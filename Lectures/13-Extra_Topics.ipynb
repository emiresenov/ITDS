{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d23bb261",
   "metadata": {
    "deletable": false
   },
   "source": [
    "# [Introduction to Data Science](http://datascience-intro.github.io/1MS041-2022/)    \n",
    "## 1MS041, 2022 \n",
    "&copy;2022 Raazesh Sainudiin, Benny Avelin. [Attribution 4.0 International     (CC BY 4.0)](https://creativecommons.org/licenses/by/4.0/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1aed5f0",
   "metadata": {},
   "source": [
    "# CountVectorizer and TFIDFVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06301a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e23f3199",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_test = np.array(['test of stuff','something of test','stuff of something'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e8b0f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "829c5ef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.fit(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e86e2839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['of', 'something', 'stuff', 'test'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8f90413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 0, 1, 1],\n",
       "        [1, 1, 0, 1],\n",
       "        [1, 1, 1, 0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.transform(X_test).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afa7bb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "738a1a99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "tfidf.fit(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c29cc81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['of', 'something', 'stuff', 'test'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53231c15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.48133417, 0.        , 0.61980538, 0.61980538],\n",
       "        [0.48133417, 0.61980538, 0.        , 0.61980538],\n",
       "        [0.48133417, 0.61980538, 0.61980538, 0.        ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_m = tfidf.transform(X_test).todense()\n",
    "tfidf_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65366ba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(tfidf_m[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3111908",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data/spam.csv',encoding='Latin-1')\n",
    "X = df['v2']\n",
    "Y = df['v1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f5f1478",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Go until jurong point, crazy.. Available only ...\n",
       "1                           Ok lar... Joking wif u oni...\n",
       "2       Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3       U dun say so early hor... U c already then say...\n",
       "4       Nah I don't think he goes to usf, he lives aro...\n",
       "                              ...                        \n",
       "5567    This is the 2nd time we have tried 2 contact u...\n",
       "5568                Will ÃŒ_ b going to esplanade fr home?\n",
       "5569    Pity, * was in mood for that. So...any other s...\n",
       "5570    The guy did some bitching but I acted like i'd...\n",
       "5571                           Rofl. Its true to its name\n",
       "Name: v2, Length: 5572, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f49131c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test, Y_train, Y_test = train_test_split(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1259c2db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28b5fddf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(tfidf.transform(X_train),Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "253758a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            labels           precision             recall\n",
      "\n",
      "              spam  0.99 : [0.83,1.00] 0.78 : [0.64,0.92]\n",
      "               ham  0.97 : [0.92,1.00] 1.00 : [0.94,1.00]\n",
      "\n",
      "          accuracy                                        0.97 : [0.92,1.00]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from Utils import classification_report_interval\n",
    "print(classification_report_interval(Y_test,lr.predict(tfidf.transform(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe15cc5",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4166bb73",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/auto.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/xp/123p26351jv0y5pbxcbx1rhc0000gn/T/ipykernel_5532/4257008770.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/auto.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 932\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1214\u001b[0m             \u001b[0;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[1;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    784\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/auto.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data/auto.csv')\n",
    "df = df.dropna()\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31e2d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['horsepower']\n",
    "Y = df['mpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bef1f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7582cd2",
   "metadata": {},
   "source": [
    "Power, vs miles per gallon.\n",
    "\n",
    "Miles per gallon is the inverse of gallon per miles which is fuel consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab69ea62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = df['horsepower'].to_numpy().reshape(-1,1)\n",
    "Y = df['mpg'].to_numpy()\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test, Y_train, Y_test = train_test_split(X,Y,random_state=0)\n",
    "lr.fit(X_train,Y_train)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(lr.predict(X_test),Y_test)\n",
    "plt.scatter(Y_test,Y_test)\n",
    "np.mean((lr.predict(X_test)-Y_test)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abe1d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = df['horsepower'].to_numpy().reshape(-1,1)\n",
    "X = np.concatenate([X,1/X],axis=1) # X = (n_samples,n_features)\n",
    "Y = df['mpg'].to_numpy()\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test, Y_train, Y_test = train_test_split(X,Y,random_state=0)\n",
    "lr.fit(X_train,Y_train)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(lr.predict(X_test),Y_test)\n",
    "plt.scatter(Y_test,Y_test)\n",
    "np.mean((lr.predict(X_test)-Y_test)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198da320",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_test[:,0],Y_test)\n",
    "plt.scatter(X_test[:,0],lr.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c686992d",
   "metadata": {},
   "source": [
    "# Improving optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990ac24d",
   "metadata": {},
   "source": [
    "1.123412e6 = 1.123412*10^6\n",
    "1.234 + 1.123412e6 = 0.000000e6 + 1.123412e6 = 1.123412e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e74718f",
   "metadata": {},
   "outputs": [],
   "source": [
    "1.23123e-300 = 1.23123*10^(-300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb27a3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as so\n",
    "import numpy as np\n",
    "data = so.loadmat('data/mammography.mat')\n",
    "\n",
    "np.random.seed(0)\n",
    "shuffle_index = np.arange(0,len(data['X']))\n",
    "np.random.shuffle(shuffle_index)\n",
    "\n",
    "X = data['X'][shuffle_index,:]\n",
    "Y = data['y'][shuffle_index,:].flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3349ffee",
   "metadata": {},
   "source": [
    "Lets make the features very different in scale and see what happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabb6a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:,0] = X[:,0] + 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a7a3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X,Y)\n",
    "lr.score(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e858d065",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "lr = LogisticRegression()\n",
    "lr.fit(sc.fit_transform(X),Y)\n",
    "lr.score(sc.transform(X),Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97812c42",
   "metadata": {},
   "source": [
    "$$\n",
    "    \\min L(w)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48890463",
   "metadata": {},
   "source": [
    "$$\n",
    "    \\min L(w) + \\lambda \\|w\\|^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6dcf0f",
   "metadata": {},
   "source": [
    "$$\n",
    "    f_w(x) = w_0 + w_1 x \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0c9cad",
   "metadata": {},
   "source": [
    "## How scale and regularization affect eachother\n",
    "\n",
    "Lets try to train two models with fairly high regularization and see what happens when we change the scale of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d3049f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['X'][shuffle_index,:]\n",
    "Y = data['y'][shuffle_index,:].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8427daee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(C=1/10000)\n",
    "lr.fit(X*100,Y)\n",
    "lr.score(X*100,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e50ecce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(C=1/10000)\n",
    "lr.fit(X,Y)\n",
    "lr.score(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f7f003",
   "metadata": {},
   "source": [
    "# Transforming target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2c9354",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "X,Y = fetch_california_housing(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5b899d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X,Y)\n",
    "lr.score(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f74067",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(lr.predict(X),Y,alpha=0.05)\n",
    "plt.scatter(lr.predict(X),lr.predict(X))\n",
    "plt.xlim(-1,10)\n",
    "plt.ylim(-1,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cf6931",
   "metadata": {},
   "outputs": [],
   "source": [
    "log(X)-log(Y) = log(X/Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7903499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "lr = LinearRegression()\n",
    "lr.fit(X,np.log(Y))\n",
    "plt.scatter(np.exp(lr.predict(X)),Y,alpha=0.05)\n",
    "plt.scatter(Y,Y)\n",
    "plt.xlim(0,7)\n",
    "plt.ylim(0,7)\n",
    "lr.score(X,np.log(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a099ac4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "lr = LinearRegression()\n",
    "lr.fit(X,np.sqrt(Y))\n",
    "plt.scatter(np.power(lr.predict(X),2),Y,alpha=0.05)\n",
    "plt.scatter(Y,Y)\n",
    "plt.xlim(0,7)\n",
    "plt.ylim(0,7)\n",
    "lr.score(X,np.sqrt(Y))\n",
    "#plt.scatter(np.exp(lr.predict(X)),np.exp(lr.predict(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5700bcc",
   "metadata": {},
   "source": [
    "We see that the largest prices seem to be hard to predict, lets see what happens if we remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a92759f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_=plt.hist(Y,bins=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88321ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "_=plt.hist(np.sqrt(Y[Y < 5]),bins=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e23be1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import power_transform\n",
    "_=plt.hist(power_transform(Y[Y < 5].reshape(-1,1)),bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9392c0e3",
   "metadata": {},
   "source": [
    "## Playing around with transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122d85e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = X[Y < 5,:]\n",
    "Y_new = Y[Y < 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9e1c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "pw = PowerTransformer()\n",
    "pw.fit(Y_new.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b751c55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_new,pw.transform(Y_new.reshape(-1,1)).flatten())\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(pw.inverse_transform(lr.predict(X_new).reshape(-1,1)),Y_new,alpha=0.05)\n",
    "plt.scatter(Y,Y)\n",
    "plt.xlim(-1,7)\n",
    "plt.ylim(-1,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d628dac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.score(X_new,pw.transform(Y_new.reshape(-1,1)).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ba36d6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pw2 = PowerTransformer()\n",
    "pw2.fit(X_new)\n",
    "lr = LinearRegression()\n",
    "lr.fit(pw2.transform(X_new),pw.transform(Y_new.reshape(-1,1)).flatten())\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(pw.inverse_transform(lr.predict(pw2.transform(X_new)).reshape(-1,1)),Y_new,alpha=0.05)\n",
    "plt.scatter(Y,Y)\n",
    "plt.xlim(-1,7)\n",
    "plt.ylim(-1,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d12bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.score(pw2.transform(X_new),pw.transform(Y_new.reshape(-1,1)).flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3483bd10",
   "metadata": {},
   "source": [
    "# Concentration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53b14a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, IntSlider, FloatSlider\n",
    "from Utils import discrete_histogram, bennett_epsilon, epsilon_bounded\n",
    "import numpy as np\n",
    "@interact \n",
    "def concentration(n=IntSlider(1,1,100,5),p=FloatSlider(value=0.5, min=0,max=1,step=0.1)):\n",
    "    import matplotlib.pyplot as plt\n",
    "    X = np.random.binomial(1,p,size=(n,10000))\n",
    "    means = np.mean(X,axis=0)\n",
    "    #print(\"P(mean > mu + 0.3 ) = %.5f <= Chebychev %.5f\" % (np.mean(means > 0.5+0.3),(1/4)/(0.3**2*n)))\n",
    "    #print(\"P(mean > mu + 0.3 ) = %.5f <= Hoeffding %.5f\" % (np.mean(means > 0.5+0.3),np.exp(-2*n*0.3**2)))\n",
    "    print(np.quantile(means,0.025),np.quantile(means,0.975))\n",
    "    epsilon1 = epsilon_bounded(n,1,0.05)\n",
    "    epsilon2 = bennett_epsilon(n,1,np.sqrt((1/2)*p*(1-p)),0.05)\n",
    "    print(\"95%% confidence interval Hoeffding [%.2f, %.2f] for n=%d\" % (np.mean(means)-epsilon1,np.mean(means)+epsilon1,n))\n",
    "    print(\"95%% confidence interval Bennett [%.2f, %.2f] for n=%d\" % (np.mean(means)-epsilon2,np.mean(means)+epsilon2,n))\n",
    "    discrete_histogram(means,normed=True)\n",
    "    plt.xlim(-0.1,1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269aded4",
   "metadata": {},
   "source": [
    "# Spam vs not spam, more complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33700059",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data/spam.csv',encoding='Latin-1')\n",
    "X = df['v2']\n",
    "Y = df['v1']\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv('data/spam.csv',encoding='Latin-1')\n",
    "\n",
    "X = df['v2']\n",
    "Y = df['v1']\n",
    "\n",
    "from Utils import train_test_validation\n",
    "\n",
    "X_train,X_test, X_valid, Y_train, Y_test, Y_valid = train_test_validation(X,Y)\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "p = Pipeline([('tfidf',TfidfVectorizer()),('model',LogisticRegression())])\n",
    "\n",
    "p.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95758ac1",
   "metadata": {},
   "source": [
    "## If we have no specific cost in mind\n",
    "\n",
    "Then simply compute confidence intervals on the standard metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbcb317",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils import classification_report_interval\n",
    "print(classification_report_interval(Y_test,p.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022723b5",
   "metadata": {},
   "source": [
    "## Lets define a cost\n",
    "\n",
    "If we say something is spam but its not spam, that is quite bad and we could miss important emails. We could say that costs $100$.\n",
    "\n",
    "If on the other hand we have spam that is classified as not spam, then that is annoying and we have to manually delete it. Lets say that incurrs a cost of $10$.\n",
    "\n",
    "That is, if we define the random variable \n",
    "$$E_1 = 1_{Y=0, g(X)=1} = 1_{Y=0} 1_{g(X) = 1}$$\n",
    "and the random variable\n",
    "$$E_2 = 1_{Y=1, g(X)=0} = 1_{Y=1} 1_{g(X) = 0} = (1-1_{Y=0})(1-1_{g(X)=1}) = 1 - 1_{Y=0} - 1_{g(X)=1} + E_1$$\n",
    "\n",
    "Then the cost of a randomly chosen sms is the random variable\n",
    "$$\n",
    "    C = 100 E_1 + 10 E_2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88644d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_01 = (Y_test == 'spam')*1 # This makes Y_01 into 0 for ham and 1 for spam\n",
    "g_01 = (p.predict(X_test) == 'spam')*1 # This makes g_01 into 0 for ham and 1 for spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65d31dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_0 = 1-Y_01\n",
    "g_1 = g_01\n",
    "E_1  = Y_0*g_1\n",
    "E_2 = 1-Y_0-g_1+E_1\n",
    "C = 100*E_1 + 10*E_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96374c37",
   "metadata": {},
   "source": [
    "We are interested in the expected (average) cost of an sms, we need to estimate $E[C]$.\n",
    "\n",
    "We assume that all the sms are i.i.d. so we can use Hoeffdings inequality:\n",
    "\n",
    "1. What do we know about $C$? Well the only thing we know is that it is bounded by $0 \\leq C \\leq 100$.\n",
    "2. Use Hoeffdings inequality to get a confidence interval\n",
    "3. We will use the `epsilon_bounded` function from `Utils` to do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20593a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils import epsilon_bounded, print_confidence_interval\n",
    "eps = epsilon_bounded(len(C),100,0.05)\n",
    "mean = np.mean(C)\n",
    "print_confidence_interval(mean,eps,min_value=0,max_value=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f0acc8",
   "metadata": {},
   "source": [
    "However, we know that we can adjust the threshold of our model, and perhaps the cost will be different for another threshold?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a118baa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(threshold):\n",
    "    Y_01 = (Y_test == 'spam')*1 # This makes Y_01 into 0 for ham and 1 for spam\n",
    "    g_01 = (p.predict_proba(X_test)[:,1] >= threshold)*1 # This makes g_01 into 0 for ham and 1 for spam\n",
    "    Y_0 = 1-Y_01\n",
    "    g_1 = g_01\n",
    "    E_1  = Y_0*g_1\n",
    "    E_2 = 1-Y_0-g_1+E_1\n",
    "    C = 100*E_1 + 10*E_2\n",
    "    \n",
    "    return np.mean(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560ea91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.linspace(0,1,100)\n",
    "costs = [cost(t) for t in thresholds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c013718",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(thresholds, np.log(costs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9964fc89",
   "metadata": {},
   "source": [
    "### Now we can compute the confidence interval in the same way, but on the validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c339629b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44053c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "lx_course_instance": "2022",
  "lx_course_name": "Introduction to Data Science",
  "lx_course_number": "1MS041"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
